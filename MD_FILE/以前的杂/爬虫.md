## http/https: 协议

#### http请求头信息：

```
	-User—Agent:请求载体的身份标识
​	-Connection：请求完毕后，是断开链接还是保持链接
常用的响应头信息
	-Content-Type:服务器响应回客户端的数据类型
```

### https：

```
安全的超文本传输协议 
加密方式
	-对称密钥加密（客户端发送数据和密钥一块发送给服务器）
	-非对称密钥加密 (服务器发送加密方式，客户端收到后按照加密方式加密数据，然后发送数据到服 务器)
	-证书密钥加密(服务器把密钥发送给第三方，第三方加上数字证书，然后给客户端 )
```

## 携带参数

```
get:params
post:data
```

## 数据解析

```
中文乱码，request完就编码一下 
response=request.get(url)
response.encoding='utf-8'
res_text=response.text

或者对每个单独的结果进行编码
result=result.encode('iso-8859-1').decode('gbk')
```

```
三种方式：正则表达式，bs4，xpath
```

```
数据解析原理概述：
	-放在标签的文本，或者属性之中
		-定义标签
		-解析标签里的文本 或者标签里面的属性
```

## request返回数据

```
text 文本数据
content 二进制的数据 图片
json 字典数据
```



## 正则表达式

```
re.findall('正则表达式',data.text,re.S|re.M) 
re.S 单行匹配
re.M 多行匹配
```

## BS4

```
soup=Beautiful(data.text,'lxml')
方法和属性:
	soup.标签名称		#只返回第一个
	soup.find(标签名称,attrs={'':''})    #返回一个找到的对象
		或者 soup.find('div',class_='some') class下面加下划线
	soup.find_all()#返回一个列表（找到的所有对象）
	soup.select('.class的属性') 
		相当于类选择器 用. id选择器用#
		soup.select('.hello') 相当于class='hello' 
		soup.select('#hello') 相当于id='hello' 
		用法2:
		soup.select('.hell > ul > li > a ')
		相当于 class=’hello‘ 下面的ul 下面的li 下面的 a 标签
		soup.select('.hell > ul  a ') # 空格代表多个层级
获取标签之间的文本数据：
	soup.a.text 或者 soup.a.string 或者 soup.a.get_text()
	text|get_text() 该标签所有的文本内容
	string:该标签直系的
获取标签中的属性值
```

### XPATH

```

最常用且最便捷高效的一种解析方式
xpath解析原理：
	-1 实例化一个etree的对象，需要将被解析的数据放到该对象中
	-2 调用etree 徐集乡中的xpath方法结合着xpath表达式实现标签的		定位和内容的捕获
	-3 实例化etree对象 from lxml import etree
		-将本地html文档中的数据加载
			etree.parse('本地路径')
		-互联网源码的数据 
			etree.HTML('page_text') 
		-xpath('xpath表达式') 
		/：表示从根节点开始做定位  
		//：表示的是多个层级表示可以在任意位置去寻找标签 
		tree.xpath('/html/body/div') 意思是:html/body/div
		tree.xpaath('/html//div')    意识是 html 下面所有div
		tree。xpath（'//div')         意思是 该页面所有div
		tree.xpath('//div[@class=“song”]') 意思是所有属性值是class='song'的div #前面有个@
		tree.xpath('//div[@class=“song”]/p[3]') 第三个p标签,从1开始
取文本
	-  /text()  只能取直系 文本 
	-  //text() 取所有的子标签下的文本
		tree.xpath('//div[@class='tang']/li[5]/a/text()')  #最后加上/text()取文本
取属性	   
	-/@attrname 
	-/@src #取属性  //@src 取所有属性
		tree.xpath('//div[@class='tang']//a/@src') 
		
```

循环 xpath 

```
from lxml import etree 
tree=etree.HTML(respon)
tree.xpath('xpath表达式')
 
for li in tree.xpath('//ul[@class='som']'/li):
	li.xpath('./div[2]/h2/a/text()')  #遍历子标签是 从 ./开始
	
用两个或者多个xpath 表达式的时候
tree.xpath('xpath表达式 ｜ xpath表达式')

```

## 验证码识别

```
验证码：反爬机制 
方法：
	-识别验证吗的操作，人工肉眼识别 不推荐
	-第三方平台（云打码）
	//*[@id="imgCode"]
	
保留日志 
```

 ## 登陆

```
登陆用post请求携带参数(用户名，密码，验证码)
验证码知识发起请求的一个参数
```

### Cookie

```
requests 是无状态发送请求
session 会记录记录 登陆状态啥的

cookie作用：让服务器端记录客户端的相关状态
cookie在headers里面：有的有有效时长
cookie来自哪里：模拟登陆post请求后，由服务器端创建
session会话对象： 
			作用：可以进行请求的发送
			如果请求过程中产生了cookie，则该cookie会被请求或者携带在session 对象中
使用session对象进行模拟登陆post请求的发送（cookie会被记录在session中）
session创建：
	session=request.Session()
```

### 代理

```
什么是代理：
	-代理服务器
代理的作用：
	-突破自身访问的限制，
	-可以隐藏自身真实的ip，免受攻击
代理相关的网站：
	-快代理
	-西次代理
	-www.goubanjia.com
```

### 异布爬虫

```
代码
from multiproce ssing.dummy import Pool

pool=Pool(4)            #4个任务一起执行
pool.map(func,可迭代对象)   #传入参数和可以迭代的对象 有返回值  
pool.close()						#关掉异步
pool.join()              #等待那啥一起关闭
```

### 单线程+异步携程

```
event_loop:事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件的循环上，当满足某							些条件的时候，函数就会被循环执行
coroutine: 携程对象，我们可以将携程对象注册到时间循环中，它会被时间循环调用，我们可以使用
						async 关键字来定义一个方法啊，这个方法在调用时不回立即被执行，而是返回一个携程						对象
		task：	任务，它是对携程对象的进一步封装，包含了任务的各个状态
	future：	代表将来执行或者还没执行的任务，实际上和task没有本质区别
	async:	定义一个携程
  await:	用来刮起阻塞方法的执行 
```

```
import asyncio                              在vs_code里回出错，需要在pycharm里跑

async def fuck(name:str):                   #async 携程修饰函数        
    print('what your name %s' % name)
    return 'done:%s' % name

c=fuck('minliang')												  # c 为携程对象
hello=asyncio.get_event_loop()
hello.run_until_complete(c)
```

#### task

```
import asyncio

async def request(url):
	print('正在那啥对象 %s' %  url)
	
a=request(url='www.baidu.com')
loop=asyncio.get_event_loop()
task=loop.create_task(c)
print(task)   #答应函数

loop.run_until_complete(task)
print(task)      打印
 
```

#### future的使用 

```
import asyncio


async def request(url):
    print(f'正在操作{url}')
    print(f'正在aaa操作{url}')
    return 'url'

d = request('www.baidu.com')
loop = asyncio.get_event_loop()
future_ = asyncio.ensure_future(d)    # future 有类创建，task 由于loop创建
loop.run_until_complete(future_)
```

#### 回调函数

```
import asyncio

async def request(url):
    print(f'正在访问url:{url}')
    print(f'访问结束url:{url}')
    return 'done %s' % url

def collback(task):      #回调函数
    print(task.result())

c=request(url='http://www.baidu.com')

loop=asyncio.get_event_loop()
task=asyncio.ensure_future(c)
task.add_done_callback(collback) # 添加回调函数,返回打印结果
loop.run_until_complete(task)

```

#### 最后的使用方法:多任务的携程

```
import asyncio
import time

start=time.time()
async def request(url):
    print(f'kaddddishi: {url}')
    await asyncio.sleep(2)     			# 异步函数内不能使用同步的方法或者函数要不然就没用，
    print(f'dondddddde:{url}')			# 本来应该用time.sleep(2)
    return f'done:{url}'

url_list = ['www.naod.com',
            'www.hello.com',
            'www.baodi.com']

def done(task):                            #回去调用函数
    print(task.result())

task_list = []
for i in url_list:
    c=request(i)
    task = asyncio.ensure_future(c)
    task.add_done_callback(do ne)          # 添加回调用函数
    task_list.append(task)

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(task_list))   #同步方法用asyncio.wait[future列表]     
print('共耗时：%d秒' % (time.time() - start))

```

### 多线程threading

```
import time
import threading

start = time.perf_counter()

def do_something(x):
    global mydict      # 要先引用全局变量
    print(f'x={x}')
    print("-> 线程启动")
    time.sleep(1)
    print("-> 线程结束")
    mydict[x]=f'{x}' *3


mydict={}
join_list=[]

for i in ['h','g','gg','\n','y','yyy']:
    d = threading.Thread(target=do_something,args=[i]) # target 函数的意思,args 可以迭代的对象
    d.start()
    join_list.append(d)

for t in join_list:             # 全部要JOIN一下
    t.join()


    
finish = time.perf_counter()

print(f"全部任务执行完成，耗时 {round(finish - start,2)} 秒")
```



### aiohttp模块 

```
基于 异布的网络请求 

import asyncio
import aiohttp

urls=['http://www.baidu.com',
      'http://www.mop.com',
      'http://www.douban.com']

async def get_page(url):
    async with aiohttp.ClientSession() as session:
        async with await session.get(url) as res:
            # 返回文本 text()
            # 返回二进制文件  read()
            # 返回Json    json()
            data = await res.text()
            print(data)

tasks=[]
for i in urls:
    c=get_page(url=i)  
    task=asyncio.ensure_future(c)
    tasks.append(task)

loop=asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
```

### selenium

#### selenium不打开浏览器

```
option = webdriver.ChromeOptions()
option.add_argument("headless")
pro = webdriver.Chrome(executable_path='./chromedriver', chrome_options=option)
```



```
方法：
	-发请求 get(url)
	-标签定位 find_element('标签名' 标签属性)
	-标签交互 send_keys('要输入的值')
					 click()   模拟点击
	-执行js程序 excute_script('jscode')
	-前进，后退。back() forword()前进
	-退出quiet
```



```
作用：
	-便捷的获取网站动加载数据
	-便捷的实现模拟登陆
```

```
from selenium import webdriver  
from lxml import etree

bro=webdriver.Chrome(executable_path='谷歌浏览器驱动')
bro.get('http://baidu.com')
page_text=bro.page_source                          #页面源码 .page_source

tree=etree.HTML(page_text)												 #实例化etree
li_list=tree.xpath('xpath表达式')
bro.quiet()																				 # 退出实例化
```

#### selenium打开淘宝

```
class='btm-hello  btm'标签有空格的时候 只需要定位左边或者右边就行了

from selenium import webdriver
from selenium.webdriver.commom.by import By #定位标签用

bro=webdriver.Chrome(executable_path='谷歌浏览器驱动' ) #放到系统环境变量里面就不用加参数了
bro.get('http://www.taobao.com')

search—input=bro.find_element(By.id,'id_name')  #找到 id=‘id_name’的标签
search.send_keys('输入要搜索的值')

btn=bro.find_element(By.CSSSELECTOR,'#classname')    #通过css选择器 应该是id为. class为#classname

bro.excute_script('window.scrollTo(0,document.body.scrollHeight)') #可以执行js代码滚屏

bro.back()				#回退 按钮
bro.forword()     #前进去
btn.click()       #点击按钮
time.sleep(2)			#停两秒
bro.quiet()				#	退出
bro.maximize_window() # 窗口最大化

元素名字.location  # 元素左上角的位子
元素名字.size      # 元素的高度和宽度
元素.text 				# 元素的文本
元素名字.get_attribute('href')  #元素的链接

```

### Selenium 处理iframe

```
如果定位的标签是存在于iframe，是定位不到的
要用如下代码
div=bro.switch_to.frame('iframeResult')   #跳转到frame witch_to_default_content()#跳转到主框架
如果要实现ifame里面的div 拖动操作
from selenium.webdriver import ActionChains
action=ActionChains(bro)#实例化动作,并且放入对象
action.click_and_hold(div)
action.move_by_offset(x,y).perform() x,y分别轴和轴的像素,perform 立刻操作
action .release()释放动作练

```

#### selenium 截图裁剪

```
有些验证码不能单独请求，要不然就不是原来的图片

bro=selenium.webdriver().Chrome('驱动')
bro.save_screenshot('pic_name')  #截图

code_img=bro.find_element(By.XPATH,'xpath表达式')
location=code_img.location #返回元素左上角的坐标
size=code_img.size  #该你元素的长和宽

PIL库是处理照片的库
from PIL import Image
i=Image.open('aa.png')
frame=i.crop('裁剪的像素') #裁剪
frame.save(code_img_name)

  
```





 

### 验证码验证平台

```
-云打码
-超级鹰 
```

### Scrapy 的使用

```
先安装好scrapy 
然后用终端进入要创建的文件夹 scrapy StartProject 项目名称
cd 项目目录 的文件夹子里
scrapy genspider spiderName www.xxx.com #创建原始爬虫文件 
执行工程 scrapy crawl spiderName
				scrapy crawl spiderName --nolog 关闭终端日志

配置文件：
ROBOTSTXT =TRUE 不允许爬   robot 协议的
LOG_LEVEL ='ERROR' 设置日志等级
USER_AGENT 你 也可以修改


```

Scrapy 数据解析

```
def parse(self,response):
	div_list=respon.xpath('.//') 和Xpath一样用
	for div in div_list:
		x=div.xpath('./text()')[0]
		y=div.xpath('xpath表达式')
		x=x.extract()								#要加一个 extract()才能获取DATA
		y=y.extract()								#要加一个 extract()才能获取DATA
		y=y.extract_first()					#只提取第一个
```

 scarpy存储内容

```
-终端指令:
	-要求，只可以将parse方法的返回值存储到本地的文本当中
		scripy crawl 文件名字  -o '本地存储路径'
-管道流程
	-数据解析
	-解析的数据封装储存到item类型的对象中
	
```







